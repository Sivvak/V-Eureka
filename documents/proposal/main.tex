\documentclass[10pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}

\geometry{
  top=0.65in,
  bottom=0.65in,
  left=1in,
  right=1in
}

\title{\textbf{Multimodal Eureka: Rewards Generated by LLM with Additional Visual Input}}
\author{
\begin{tabular}{cc}
     \textbf{Łukasz Łopacki} & \textbf{Szymon Pobłocki} \\
     Unversity of Warsaw & University of Warsaw \\
     ll439936@students.mimuw.edu.pl & s.poblocki@student.uw.edu.pl
     \\\\
     \textbf{Paweł Siwak} & \textbf{Jakub Winiarski} \\
     Unversity of Warsaw & University of Warsaw \\
     pl.siwak@student.uw.edu.pl & jj.winiarski@student.uw.edu.pl
     \\\\
     \textbf{Konrad Staniszewski\footnote{First supervisor}}  & \textbf{Paweł Budzianowski\footnote{Second supervisor}}\\
     University of Warsaw & University of Warsaw\\
     ks.staniszewski@uw.edu.pl & budzianowski@gmail.com
\end{tabular}
}
\date{\today}

\begin{document}

\maketitle

\begin{center}
    {\Large\bfseries Abstract}
\end{center}

\begin{center}
\begin{minipage}{0.90\linewidth}
\small

Eureka is a recent framework where large language models generate reward functions for reinforcement learning agents using only code-level environment descriptions. We want to extend this approach by introducing \textbf{Multimodal Eureka}, which supplements the model with visual input to improve its intuitive understanding of tasks. This mimics how humans better comprehend situations when both visual and textual information are available. We plan on evaluating our method on several control environments, assesing whether visual context can lead to higher-quality rewards and improved success rates in complex tasks.

\end{minipage}
\end{center}

\section{Introduction}
Large language models have demonstrated impressive capabilities as semantic planners for high-level decision-making tasks. Recently, effort has been made to utilise them for low-level motor control—such as dexterous manipulation and precision tasks—remains. One example of such work is the Eureka framework \cite{eureka} that addresses part of this gap by using LLMs to automatically generate and evolve reward functions through in-context learning and evolutionary optimization.

Eureka has been shown to outperform human-crafted rewards on 83\% of 29 benchmark tasks, including several involving hand-based control, without requiring task-specific templates or prior tuning. Despite its success, Eureka operates exclusively on internal state inputs (e.g., joint angles, velocities), which limits its effectiveness in tasks where visual perception plays a central role.

In many real-world robotic applications, visual input is essential for interacting with objects in unstructured environments, resolving occlusions, and aligning actions based on external context. To address this limitation, we propose to extend Eureka with visual capabilities—enabling it to generate and refine reward functions based on visual observations in addition to internal state.

We call this extension \textbf{Multimodal Eureka}. Our approach augments the reward design process with visual inputs by leveraging multimodal large language models capable of interpreting both code and image data. While most relevant information can theoretically be inferred from environment code and variable names, we hypothesize that visual representations help the model build a more intuitive understanding of the task—similarly to how a human benefits from seeing a situation rather than imagining it solely from description. We hypothesize that this added context can improve the model’s ability to generate more aligned and effective reward functions in tasks where spatial reasoning or object interaction plays a key role.


\section{Related Work}
Recently, increasingly powerful large language models are being developed, like OpenAI GPT-4 \cite{GPT-4} or Gemini \cite{Gemini}. One area in which such models can be utilized is robot control. In a work done at Microsoft \cite{Microsoft}, a model is prompted to steer a robot via generated code to accomplish a given task, with model having a textual description of the environment at its disposal. 

LLMs like GPT-4 now often have auxiliary capabilities, such as also accepting additional image data, which is utilized to increase the accuracy of LLMs in robot control tasks. For instance, the work \cite{TUDelft}, is a simple expansion of the previously quoted Microsoft paper, while a more recent article in Nature \cite{Nature} is a more advanced framework for steering robots, also utilizing GPT-4 with image data.

A different way to utilize LLMs for such tasks is to have them design rewards for Reinforcement Learning programs, rather than steer the robot directly. Since the publication of the seminal Eureka paper \cite{eureka}, which describes a framework for reward design many new frameworks aiming to alleviate some of Eureka's flaws have been proposed. They include frameworks such as CARD \cite{card}, which attempts to minimize token usage in comparison with Eureka, or Text2Reward \cite{Text2Reward}, which writes code for rewards directly instead of using APIs. However, to our knowledge, no attempts to supplement Eureka with image data have been made, which appears to us to be a worthwhile endeavour. 

\section{Experiments}
The main goal of our project is to build upon ideas presented in the \textit{Eureka} paper~\cite{eureka}. We would like to leverage the multimodal abilities of current SoTA LLMs and try to get even higher efficiency than in the referenced paper.

In the original work, \textit{Eureka} was compared with L2R~\cite{l2r}, human-written rewards created by active reinforcement learning researchers, and sparse rewards (success/failure). \textbf{The main purpose of our experiments is to compare \textit{Multimodal Eureka} with the original \textit{Eureka} itself.} Because \textit{Eureka} has already been compared with the aforementioned baselines, we do not see a reason to check other baselines. We aim to achieve better results than in the original paper by leveraging the power of computer vision, particularly in terms of spatial understanding.

To check the results, we will conduct a series of experiments.

\begin{itemize}
    \item We will calculate \textit{Success Rate} for both algorithms for multiple environments. However, we probably won't include those used in the original paper (e.g. \textit{Isaac Gym}~\cite{isaac_gym}), as they're quite old, no longer maintained, and therefore hard to use. Instead, we'll try to focus on leveraging the \textit{K-Sim}~\cite{ksim_repo} RL training library for humanoid locomotion and manipulation.
    \item We will test different types of prompts for image analysis in \textit{Multimodal Eureka} and choose the best one in terms of the average \textit{Success Rate}.
    \item The experiments will probably require a large model, so our choice of LLM will depend on what we can access. We will consider using multimodal \textit{Gemini} and \textit{GPT} models.
    \item We will experiment with different numbers of input images (e.g. last few frames).
    \item We will check the impact of image quality on the \textit{Success Rate}.
\end{itemize}

Our goal is to find the best possible solution. The primary challenge lies in creating prompts that will enable the model to extract the most crucial information from the images. The second challenge is to make the solution as general as possible. The original \textit{Eureka} uses the same set of prompts for each environment, and we would like to fulfill this requirement too.

To explain to the reader what the aim of \textit{image prompts} is, let us show an example of what a simplified version of a prompt could look like:
\\\\
\texttt{Based on an input image and instructions from the initial\_system prompt~\footnote{initial\_system prompt is a fragment from source code repository~\cite{eureka_repo} describing an agent his role and expected output}, create an optimal reward function, which will allow the agent to solve the environment efficiently. Take into consideration the laws of physics and try to connect variable names from the source code with what you see in the provided pictures.}
\\\\
Different prompts can lead to various unexpected behaviors. Therefore, we will try to identify such cases and list the most interesting observations. If we get meaningful results for situations with a clear physical explanation, we will list them in the final report as well.


\begin{thebibliography}{9}
\bibitem{eureka}
Yecheng Jason Ma and William Liang and Guanzhi Wang and De-An Huang and Osbert Bastani and Dinesh Jayaraman and Yuke Zhu and Linxi Fan and Anima Anandkumar, \emph{Eureka: Human-Level Reward Design via Coding Large Language Models}. arXiv preprint: Arxiv-2310.12931, 2023
\bibitem{l2r}
Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. \emph{Language to rewards for robotic skill synthesis}. arXiv preprint arXiv:2306.08647, 2023.
\bibitem{isaac_gym}
Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin,
David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. \emph{Isaac gym: High performance
gpu-based physics simulation for robot learning}. arXiv preprint arXiv:2108.10470, 2021.
\bibitem{eureka_repo}
Yecheng Jason Ma and William Liang, \emph{Eureka Repository on GitHub}, https://github.com/eureka-research/Eureka

\bibitem{ksim_repo}
K-Scale Labs, \emph{K-Sim Repository on GitHub}
https://github.com/kscalelabs/ksim

\bibitem{GPT-4}
OpenAI,
\href{https://arxiv.org/abs/2303.08774}{\emph{Gpt-4 technical report}},
Preprint,
arXiv:2303.08774.

\bibitem{Microsoft}
N. Wake, A. Kanehira, K. Sasabuchi, J. Takamatsu and K. Ikeuchi, 
\href{https://ieeexplore.ieee.org/document/10235949}{\emph{ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application}},
in IEEE Access, vol. 11, pp. 95060-95078, 2023, 
doi: 10.1109/ACCESS.2023.3310935.

\bibitem{Nature}
Mon-Williams, R., Li, G., Long, R. et al. 
\href{https://doi.org/10.1038/s42256-025-01005-x}{\emph{Embodied large language models enable robots to complete complex tasks in unpredictable environments.}}
Nat Mach Intell 7, 592–601 (2025). 

\bibitem{TUDelft}
L.E. Verbaan et al.
\href{https://repository.tudelft.nl/record/uuid:2944a8e1-0a54-492b-af63-193a7ac11db8#title}{Perception and Control with Large Language Models in Robotic Manipulation}
Master Thesis at TUDelft, 2024

\bibitem{Gemini}
Gemini Team Google,
\href{https://arxiv.org/abs/2312.11805}{\emph{Gemini: A Family of Highly Capable Multimodal Models}},
arXiv:2312.11805

\bibitem{card}
Shengjie Sun, Runze Liu, Jiafei Lyu, Jing-Wen Yang, Liangpeng Zhang, Xiu Li,
\href{https://doi.org/10.48550/arXiv.2410.14660}{\emph{A Large Language Model-Driven Reward Design Framework via Dynamic Feedback for Reinforcement Learning}.}
arXiv:2410.14660, 2024

\bibitem{Text2Reward}
Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu,
\href{https://doi.org/10.48550/arXiv.2309.11489}{\emph{Text2Reward: Reward Shaping with Language Models for Reinforcement Learning}},
arXiv:2309.11489 2023,

\end{thebibliography}

\end{document}