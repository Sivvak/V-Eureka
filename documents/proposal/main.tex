\documentclass[10pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}

\geometry{
  top=0.65in,
  bottom=0.65in,
  left=1in,
  right=1in
}

\title{\textbf{Multimodal Eureka: Rewards Generated by LLM with Additional Visual Input}}
\author{
\begin{tabular}{cc}
     \textbf{Łukasz Łopacki} & \textbf{Szymon Pobłocki} \\
     Unversity of Warsaw & University of Warsaw \\
     ll439936@students.mimuw.edu.pl & sp438679@students.mimuw.edu.pl
     \\\\
     \textbf{Paweł Siwak} & \textbf{Jakub Winiarski} \\
     Unversity of Warsaw & University of Warsaw \\
     pl.siwak@student.uw.edu.pl & jj.winiarski@student.uw.edu.pl
\end{tabular}
\\\\
\textbf{Konrad Staniszewski\footnote{Project supervisor}} \\
University of Warsaw \\
ks.staniszewski@uw.edu.pl
}
\date{\today}

\begin{document}

\maketitle

\begin{center}
    {\Large\bfseries Abstract}
\end{center}

\begin{center}
\begin{minipage}{0.90\linewidth}
\small

...

\end{minipage}
\end{center}

\section{Introduction}
...

\section{Related Work}
...

\section{Experiments}
The main goal of our project is to build upon ideas presented in the Eureka paper~\cite{eureka}. We would like to leverage multimodal abilities of current SoTA LLMs and try to get even higher efficiency than in the referenced paper.

In the original work, Eureka was compared to L2R~\cite{l2r}, human-written rewards created by active reinforcement learning researchers and sparse  rewards (success/fail). \textbf{The main purpose of our experiments is to compare multimodal Eureka to original Eureka itself.} Because Eureka has already been compared to the most important baselines, we do not see a reason to check other baselines. Our goal is to get better results than in original paper by using another modality.

To check the results,, we will conduct a series of experiments.

\begin{itemize}
    \item We will calculate \textit{Success Rate} for both algorithms for multiple environments including the ones included in the original paper (e.g. \textit{Isaac Gym}~\cite{isaac_gym}).
    \item We will test different types of prompts for image analysis in \textit{Multimodal Eureka} and choose the best one in terms of average \textit{ success rate}.
    \item The experiments will probably require a large model, so the choice of LLM depends on what we can access. We will consider using multimodal \textit{Gemini} and \textit{GPT} models.
    \item We will experiment with different number of input of images (e.g. last few frames).
    \item We will check the impact of image quality on \textit{Success Rate}.
\end{itemize}

We would like to finish with the best possible solution. The main challenge will be to generate prompts that will allow the model to extract the most important information from the images. The second challenge is to make the solution as general as possible. Original \textit{Eureka} uses the same prompts for each environment, and we would like to fulfill this requirement.

To explain to the reader what is the aim of \textit{image prompts}, let us show an example of what a simplified version of prompt could look like:
\\\\
\texttt{Based on input image and instructions from initial\_system prompt~\footnote{initial\_system prompt is a fragment from source code repository~\cite{eureka_repo} describing an agent his role and expected output}, try to create a reward function. Take into consideration physics laws and try to connect variable names from source code with what you see in the picture.}
\\\\
Different prompts can lead to unexpected behaviors and we will list the most interesting observations. If we get meaningful results for situations with clear physcial explanation, we will list them in final report as well.


\begin{thebibliography}{9}
\bibitem{eureka}
Yecheng Jason Ma and William Liang and Guanzhi Wang and De-An Huang and Osbert Bastani and Dinesh Jayaraman and Yuke Zhu and Linxi Fan and Anima Anandkumar, \emph{Eureka: Human-Level Reward Design via Coding Large Language Models}. arXiv preprint: Arxiv-2310.12931, 2023
\bibitem{l2r}
Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. \emph{Language to rewards for robotic skill synthesis}. arXiv preprint arXiv:2306.08647, 2023.
\bibitem{isaac_gym}
Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin,
David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. \emph{Isaac gym: High performance
gpu-based physics simulation for robot learning}. arXiv preprint arXiv:2108.10470, 2021.
\bibitem{eureka_repo}
Yecheng Jason Ma and William Liang, \emph{Eureka Repository on GitHub}, https://github.com/eureka-research/Eureka
\end{thebibliography}

\end{document}
